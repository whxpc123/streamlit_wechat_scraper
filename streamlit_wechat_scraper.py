import time
import pandas as pd
import streamlit as st
import requests
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
from io import BytesIO

# Streamlit é¡µé¢é…ç½®
st.title('WeChat Article Scraper')
keyword = st.text_input('Enter search keyword', 'AIç»˜ç”»')
num_pages = st.number_input('Enter number of pages to scrape', min_value=1, max_value=20, value=5)
start_button = st.button('Start Scraping')

def download_image(img_url, image_count):
    try:
        # æ£€æŸ¥æ˜¯å¦æ˜¯base64ç¼–ç çš„å›¾ç‰‡
        if img_url.startswith('data:image'):
            # æå–base64æ•°æ®
            base64_data = img_url.split(',')[1]
            img_data = base64.b64decode(base64_data)
            # ä¿å­˜å›¾ç‰‡
            with open(f"AIGC/{image_count}.jpg", "wb") as file:
                file.write(img_data)
        else:
            # ä¸‹è½½æ™®é€šå›¾ç‰‡
            response = requests.get(img_url, timeout=10)
            if response.status_code == 200:
                img_data = response.content
                # éªŒè¯å›¾ç‰‡æ•°æ®ï¼ˆä½¿ç”¨å¸¸è§çš„å›¾ç‰‡æ ¼å¼è¿›è¡ŒéªŒè¯ï¼‰
                if not img_data.startswith(b'\xff\xd8') and not img_data.endswith(b'\xff\xd9') and \
                   not img_data.startswith(b'\x89PNG') and not img_data.endswith(b'IEND\xaeB`\x82'):
                    raise DownloadException("Invalid image data")
                # ä¿å­˜å›¾ç‰‡
                with open(f"AIGC/{image_count}.jpg", "wb") as file:
                    file.write(img_data)
            else:
                raise DownloadException(f"Failed to download image {image_count}: HTTP {response.status_code}")
    except Exception as e:
        with open("error_log.txt", "a") as log_file:
            log_file.write(f"Failed to download image {image_count}: {str(e)}\n")
        raise DownloadException(f"Failed to download image {image_count}: {str(e)}")

if start_button:
    st.write(f'Starting to scrape articles for: {keyword}')

    # æ„é€ æœç´¢ URL
    search_url = f"https://weixin.sogou.com/weixin?type=2&query={keyword}&ie=utf8"

    data = []
    try:
        for page in range(1, num_pages + 1):
            url = f"{search_url}&page={page}"
            response = requests.get(url)
            if response.status_code != 200:
                st.error(f"Failed to retrieve search results: HTTP {response.status_code}")
                raise Exception(f"Failed to retrieve search results: HTTP {response.status_code}")

            soup = BeautifulSoup(response.text, 'html.parser')
            articles = soup.find_all('div', class_='txt-box')

            for index, article in enumerate(articles):
                try:
                    st.write(f"Processing article {index + 1} on page {page}")
                    title_element = article.find('h3')
                    title = title_element.text
                    link = title_element.find('a')['href']
                    summary = article.find('p', class_='txt-info').text

                    # æœ‰äº›æ–‡ç« å¯èƒ½æ²¡æœ‰æ¥æºä¿¡æ¯ï¼Œéœ€è¦è¿›è¡Œæ£€æŸ¥
                    source_element = article.find('div', class_='s-p')
                    source = source_element.text if source_element else 'N/A'

                    data.append({
                        'Title': title,
                        'Summary': summary,
                        'Link': link,
                        'Source': source
                    })
                except Exception as e:
                    st.write(f"Error extracting article {index + 1} on page {page}: {e}")

    except Exception as e:
        st.error(f"Error occurred: {str(e)}")

    # ä¿å­˜æ•°æ®åˆ°Excelæ–‡ä»¶
    current_time = time.strftime("%Y%m%d%H%M%S")
    file_name = f"AI_å¾®ä¿¡_{current_time}.xlsx"
    df = pd.DataFrame(data)

    # å°† DataFrame ä¿å­˜åˆ° BytesIO å¯¹è±¡ä¸­
    towrite = BytesIO()
    df.to_excel(towrite, index=False, engine='openpyxl')
    towrite.seek(0)

    # æä¾›æ–‡ä»¶ä¸‹è½½é“¾æ¥
    st.download_button(label='ğŸ“¥ Download Excel File',
                       data=towrite,
                       file_name=file_name,
                       mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet')

    st.write('Scraping completed! You can download the results as an Excel file.')
